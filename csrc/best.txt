#include <torch/torch.h>
#include <hip/hip_runtime.h>
#include <hip/hip_bfloat16.h> 
#include <ATen/hip/HIPContext.h> 
#include <vector>
#include <cmath>

using torch::Tensor;

#define WARP_SIZE 64

// Warp Reduction cho Max và Sum
__device__ __forceinline__ float warp_reduce_max(float val) {
    for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2)
        val = fmaxf(val, __shfl_xor(val, offset, WARP_SIZE));
    return val;
}

__device__ __forceinline__ float warp_reduce_sum(float val) {
    for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2)
        val += __shfl_xor(val, offset, WARP_SIZE);
    return val;
}

// -------------------------------------------------------------------------
// Kernel V100: Flash-Optimized Paged Attention
// -------------------------------------------------------------------------
__global__ void paged_attn_v100_kernel(
    const hip_bfloat16* __restrict__ query,       
    const hip_bfloat16* __restrict__ key_cache,   
    const hip_bfloat16* __restrict__ value_cache, 
    const int* __restrict__ block_tables,         
    const int* __restrict__ kv_lens,              
    const int* __restrict__ q_lens,           
    const int* __restrict__ q_start_loc,      
    hip_bfloat16* __restrict__ output,            
    const int num_kv_heads, const int num_q_heads, const int block_size,
    const float scale, const float soft_cap, const int head_size,
    const int max_blocks_per_seq, const int num_seqs,
    const long long q_s0, const long long q_s1, const long long q_s2,
    const long long k_s0, const long long k_s1, const long long k_s2, const long long k_s3,
    const long long v_s0, const long long v_s1, const long long v_s2, const long long v_s3
) {
    // Shared memory layout
    extern __shared__ float s_mem[]; 
    float* s_query = s_mem;                               // [head_size]
    float* s_max   = &s_mem[head_size];                   // [32 warps]
    float* s_sum   = &s_mem[head_size + 32];              // [32 warps]
    float* s_acc_combine = &s_mem[head_size + 64];        // [32 warps * head_size]

    const int q_token_idx = blockIdx.x; 
    const int q_head_idx  = blockIdx.y; 
    const int tid         = threadIdx.x;
    const int wid         = tid / WARP_SIZE;
    const int lane        = tid % WARP_SIZE;
    const int num_warps   = blockDim.x / WARP_SIZE;

    // 1. Xác định sequence index
    int seq_idx = 0;
    for (int i = 0; i < num_seqs; i++) {
        if (q_token_idx >= q_start_loc[i]) seq_idx = i;
    }
    
    const int q_pos = q_token_idx - q_start_loc[seq_idx];
    const int kv_len = kv_lens[seq_idx];
    const int q_len = q_lens[seq_idx];
    const int kv_head_idx = q_head_idx / (num_q_heads / num_kv_heads);
    const int causal_limit = kv_len - q_len + q_pos;

    // 2. Load Query vào Shared Memory
    if (tid < head_size) {
        s_query[tid] = static_cast<float>(query[q_token_idx * q_s0 + q_head_idx * q_s1 + tid * q_s2]);
    }
    __syncthreads();

    // 3. Xử lý song song Context qua các Warp
    float m_i = -1e20f;
    float d_i = 0.0f;
    float acc_local[8] = {0.0f}; // Mỗi thread quản lý tối đa 8 phần tử của head_size (8*64=512)
    const int elements_per_thread = (head_size + WARP_SIZE - 1) / WARP_SIZE;

    for (int i = wid; i < kv_len; i += num_warps) {
        if (i > causal_limit) break;

        const int p_blk = block_tables[seq_idx * max_blocks_per_seq + (i / block_size)];
        const int blk_off = i % block_size;

        // A. Dot Product
        float local_dot = 0.0f;
        for (int e = 0; e < elements_per_thread; e++) {
            int d = lane + e * WARP_SIZE;
            if (d < head_size) {
                float k_val = static_cast<float>(key_cache[p_blk * k_s0 + blk_off * k_s1 + kv_head_idx * k_s2 + d * k_s3]);
                local_dot += s_query[d] * k_val * scale;
            }
        }
        float score = warp_reduce_sum(local_dot);
        score = __shfl(score, 0, WARP_SIZE); // Broadcast score trong warp

        if (soft_cap > 0.0f) score = soft_cap * tanhf(score / soft_cap);

        // B. Online Softmax Update
        float m_next = fmaxf(m_i, score);
        float exp_score = expf(score - m_next);
        float exp_old = expf(m_i - m_next);

        m_i = m_next;
        d_i = d_i * exp_old + exp_score;

        for (int e = 0; e < elements_per_thread; e++) {
            int d = lane + e * WARP_SIZE;
            if (d < head_size) {
                float v_val = static_cast<float>(value_cache[p_blk * v_s0 + blk_off * v_s1 + kv_head_idx * v_s2 + d * v_s3]);
                acc_local[e] = acc_local[e] * exp_old + exp_score * v_val;
            }
        }
    }

    // 4. Gộp kết quả các Warp (Inter-Warp Reduction)
    if (lane == 0) {
        s_max[wid] = m_i;
        s_sum[wid] = d_i;
    }
    for (int e = 0; e < elements_per_thread; e++) {
        int d = lane + e * WARP_SIZE;
        if (d < head_size) s_acc_combine[wid * head_size + d] = acc_local[e];
    }
    __syncthreads();

    // 5. Warp 0 tính kết quả cuối cùng
    if (wid == 0) {
        float global_max = -1e20f;
        for (int w = 0; w < num_warps; w++) global_max = fmaxf(global_max, s_max[w]);

        float global_sum = 0.0f;
        for (int w = 0; w < num_warps; w++) {
            global_sum += s_sum[w] * expf(s_max[w] - global_max);
        }

        for (int e = 0; e < elements_per_thread; e++) {
            int d = lane + e * WARP_SIZE;
            if (d < head_size) {
                float final_acc = 0.0f;
                for (int w = 0; w < num_warps; w++) {
                    final_acc += s_acc_combine[w * head_size + d] * expf(s_max[w] - global_max);
                }
                output[q_token_idx * num_q_heads * head_size + q_head_idx * head_size + d] = 
                    static_cast<hip_bfloat16>(final_acc / (global_sum + 1e-10f));
            }
        }
    }
}

// -------------------------------------------------------------------------
// Host Code
// -------------------------------------------------------------------------
Tensor custom_paged_attn(
    const Tensor& query, const Tensor& key_cache, const Tensor& value_cache,
    const std::vector<int64_t>& query_lens, const std::vector<int64_t>& kv_lens,
    Tensor block_tables, double scale, double soft_cap = -1.0
) {
    auto device = query.device();
    const int num_seqs = query_lens.size();
    const int total_q_tokens = query.size(0);
    const int head_size = query.size(2);
    const int num_q_heads = query.size(1);

    auto opts = torch::TensorOptions().dtype(torch::kInt32).device(device);
    
    std::vector<int> h_q_start(num_seqs);
    int curr = 0;
    for(int i=0; i<num_seqs; ++i) { h_q_start[i] = curr; curr += query_lens[i]; }

    Tensor q_start_gpu = torch::tensor(h_q_start, opts);
    Tensor kv_lens_gpu = torch::tensor(std::vector<int>(kv_lens.begin(), kv_lens.end()), opts);
    Tensor q_lens_gpu  = torch::tensor(std::vector<int>(query_lens.begin(), query_lens.end()), opts);

    Tensor output = torch::empty_like(query);

    // TỐI ƯU BS1: Sử dụng nhiều Warp (8 warps = 512 threads) để lấp đầy CU
    int warps_per_block = 8;
    dim3 grid(total_q_tokens, num_q_heads);
    dim3 block(warps_per_block * WARP_SIZE); 

    // Shmem: head_size + max_warps*2 + max_warps*head_size
    size_t shmem = (head_size + 64 + 8 * head_size) * sizeof(float);

    hipStream_t stream = at::hip::getCurrentHIPStream().stream();

    paged_attn_v100_kernel<<<grid, block, shmem, stream>>>(
        (const hip_bfloat16*)query.data_ptr(), (const hip_bfloat16*)key_cache.data_ptr(), (const hip_bfloat16*)value_cache.data_ptr(),
        block_tables.data_ptr<int>(), kv_lens_gpu.data_ptr<int>(), q_lens_gpu.data_ptr<int>(),
        q_start_gpu.data_ptr<int>(), (hip_bfloat16*)output.data_ptr(),
        (int)key_cache.size(2), num_q_heads, (int)key_cache.size(1), (float)scale, (float)soft_cap, head_size, 
        (int)block_tables.size(1), num_seqs,
        query.stride(0), query.stride(1), query.stride(2),
        key_cache.stride(0), key_cache.stride(1), key_cache.stride(2), key_cache.stride(3),
        value_cache.stride(0), value_cache.stride(1), value_cache.stride(2), value_cache.stride(3)
    );
    
    return output;
}

PYBIND11_MODULE(custom_attn, m) {
    m.def("custom_paged_attn", &custom_paged_attn, "Ultimate Paged Attention V100");
}




























#include <torch/torch.h>
#include <hip/hip_runtime.h>
#include <hip/hip_bfloat16.h> 
#include <ATen/hip/HIPContext.h> 
#include <vector>
#include <cmath>

using torch::Tensor;

#define WARP_SIZE 64

// -------------------------------------------------------------------------
// Helper: Khử (Reduction) toàn Block chuẩn xác
// -------------------------------------------------------------------------
__device__ __forceinline__ float block_reduce_sum_v170(float val, float* shared_res) {
    int tid = threadIdx.x;
    int wid = tid / WARP_SIZE;
    int lane = tid % WARP_SIZE;
    for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2) val += __shfl_xor(val, offset, WARP_SIZE);
    if (lane == 0) shared_res[wid] = val;
    __syncthreads();
    val = (tid < blockDim.x / WARP_SIZE) ? shared_res[tid] : 0.0f;
    if (wid == 0) {
        for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2) val += __shfl_xor(val, offset, WARP_SIZE);
    }
    if (tid == 0) shared_res[0] = val;
    __syncthreads();
    return shared_res[0];
}

// -------------------------------------------------------------------------
// Kernel 1: Split-KV Compute (Dành cho Context dài để tăng throughput)
// -------------------------------------------------------------------------
__global__ void paged_attn_split_v170(
    const hip_bfloat16* __restrict__ q, const hip_bfloat16* __restrict__ k_cache, const hip_bfloat16* __restrict__ v_cache,
    const int* __restrict__ block_tables, const int* __restrict__ kv_lens, const int* __restrict__ q_lens, const int* __restrict__ q_start_loc,
    float* __restrict__ partial_acc, float* __restrict__ partial_max, float* __restrict__ partial_sum,
    const int num_splits, const int num_kv_heads, const int num_q_heads, const int head_size, const int block_size,
    const float scale, const float soft_cap, const int max_blocks_per_seq, const int num_seqs,
    const long long q_s0, const long long q_s1, const long long q_s2,
    const long long k_s0, const long long k_s1, const long long k_s2, const long long k_s3,
    const long long v_s0, const long long v_s1, const long long v_s2, const long long v_s3
) {
    const int q_token_idx = blockIdx.x; const int head_idx = blockIdx.y; const int split_idx = blockIdx.z; const int tid = threadIdx.x;
    extern __shared__ float s_mem[];
    float* s_query = s_mem; float* s_res = &s_mem[head_size];

    int seq_idx = 0; for (int i = 0; i < num_seqs; i++) { if (q_token_idx >= q_start_loc[i]) seq_idx = i; }
    const int kv_len = kv_lens[seq_idx]; const int q_pos = q_token_idx - q_start_loc[seq_idx];
    const int causal_limit = kv_len - q_lens[seq_idx] + q_pos;

    if (tid < head_size) s_query[tid] = static_cast<float>(q[q_token_idx * q_s0 + head_idx * q_s1 + tid * q_s2]);
    __syncthreads();

    const int tokens_per_split = (kv_len + num_splits - 1) / num_splits;
    const int start_token = split_idx * tokens_per_split;
    const int end_token = min(start_token + tokens_per_split, causal_limit + 1);

    if (start_token >= end_token) return;

    float m_i = -1e20f, d_i = 0.0f, acc_val = 0.0f;
    const int kv_head_idx = head_idx / (num_q_heads / num_kv_heads);

    for (int i = start_token; i < end_token; i++) {
        const int p_blk = block_tables[seq_idx * max_blocks_per_seq + (i / block_size)];
        const int blk_off = i % block_size;
        float local_dot = (tid < head_size) ? (s_query[tid] * static_cast<float>(k_cache[p_blk * k_s0 + blk_off * k_s1 + kv_head_idx * k_s2 + tid * k_s3]) * scale) : 0.0f;
        float score = block_reduce_sum_v170(local_dot, s_res);
        if (soft_cap > 0.0f) score = soft_cap * tanhf(score / soft_cap);
        float m_next = fmaxf(m_i, score), p = expf(score - m_next), alpha = expf(m_i - m_next);
        m_i = m_next; d_i = d_i * alpha + p;
        if (tid < head_size) acc_val = acc_val * alpha + p * static_cast<float>(v_cache[p_blk * v_s0 + blk_off * v_s1 + kv_head_idx * v_s2 + tid * v_s3]);
    }
    if (tid < head_size) partial_acc[((q_token_idx * num_q_heads + head_idx) * num_splits + split_idx) * head_size + tid] = acc_val;
    if (tid == 0) {
        int base = (q_token_idx * num_q_heads + head_idx) * num_splits + split_idx;
        partial_max[base] = m_i; partial_sum[base] = d_i;
    }
}

// -------------------------------------------------------------------------
// Kernel 2: Combine
// -------------------------------------------------------------------------
__global__ void paged_attn_combine_v170(
    const float* __restrict__ partial_acc, const float* __restrict__ partial_max, const float* __restrict__ partial_sum,
    hip_bfloat16* __restrict__ output, const int num_splits, const int head_size, const int num_q_heads
) {
    const int q_token_idx = blockIdx.x; const int head_idx = blockIdx.y; const int tid = threadIdx.x;
    if (tid >= head_size) return;
    const int base_idx = (q_token_idx * num_q_heads + head_idx) * num_splits;
    float global_max = -1e20f;
    for (int s = 0; s < num_splits; s++) global_max = fmaxf(global_max, partial_max[base_idx + s]);
    float global_sum = 0.0f, final_acc = 0.0f;
    for (int s = 0; s < num_splits; s++) {
        float m_s = partial_max[base_idx + s]; if (m_s < -1e10f) continue;
        float alpha = expf(m_s - global_max);
        global_sum += alpha * partial_sum[base_idx + s];
        final_acc += alpha * partial_acc[(base_idx + s) * head_size + tid];
    }
    output[q_token_idx * num_q_heads * head_size + head_idx * head_size + tid] = static_cast<hip_bfloat16>(final_acc / (global_sum + 1e-10f));
}

// -------------------------------------------------------------------------
// Kernel 3: Single-Pass (Dành cho BS1 Context ngắn để giảm latency)
// -------------------------------------------------------------------------
__global__ void paged_attn_single_v170(
    const hip_bfloat16* __restrict__ q, const hip_bfloat16* __restrict__ k_cache, const hip_bfloat16* __restrict__ v_cache,
    const int* __restrict__ block_tables, const int scalar_kv_len, const int scalar_q_len,
    hip_bfloat16* __restrict__ output, const int num_kv_heads, const int num_q_heads, const int head_size, const int block_size,
    const float scale, const float soft_cap, const int max_blocks_per_seq,
    const long long q_s0, const long long q_s1, const long long q_s2,
    const long long k_s0, const long long k_s1, const long long k_s2, const long long k_s3,
    const long long v_s0, const long long v_s1, const long long v_s2, const long long v_s3
) {
    const int head_idx = blockIdx.y; const int tid = threadIdx.x;
    extern __shared__ float s_mem[];
    float* s_query = s_mem; float* s_res = &s_mem[head_size];

    const int causal_limit = scalar_kv_len - scalar_q_len;
    if (tid < head_size) s_query[tid] = static_cast<float>(q[head_idx * q_s1 + tid * q_s2]);
    __syncthreads();

    float m_i = -1e20f, d_i = 0.0f, acc_val = 0.0f;
    const int kv_head_idx = head_idx / (num_q_heads / num_kv_heads);

    for (int i = 0; i < scalar_kv_len; i++) {
        if (i > causal_limit) break;
        const int p_blk = block_tables[i / block_size];
        const int blk_off = i % block_size;
        float local_dot = (tid < head_size) ? (s_query[tid] * static_cast<float>(k_cache[p_blk * k_s0 + blk_off * k_s1 + kv_head_idx * k_s2 + tid * k_s3]) * scale) : 0.0f;
        float score = block_reduce_sum_v170(local_dot, s_res);
        if (soft_cap > 0.0f) score = soft_cap * tanhf(score / soft_cap);
        float m_next = fmaxf(m_i, score), p = expf(score - m_next), alpha = expf(m_i - m_next);
        m_i = m_next; d_i = d_i * alpha + p;
        if (tid < head_size) acc_val = acc_val * alpha + p * static_cast<float>(v_cache[p_blk * v_s0 + blk_off * v_s1 + kv_head_idx * v_s2 + tid * v_s3]);
    }
    if (tid < head_size) output[head_idx * head_size + tid] = static_cast<hip_bfloat16>(acc_val / (d_i + 1e-10f));
}

// -------------------------------------------------------------------------
// Host Code
// -------------------------------------------------------------------------
Tensor custom_paged_attn(
    const Tensor& query, const Tensor& key_cache, const Tensor& value_cache,
    const std::vector<int64_t>& query_lens, const std::vector<int64_t>& kv_lens,
    Tensor block_tables, double scale, double soft_cap = -1.0
) {
    auto device = query.device(); const int num_seqs = query_lens.size();
    const int total_q_tokens = query.size(0); const int num_heads = query.size(1); const int head_size = query.size(2);
    Tensor output = torch::empty_like(query);
    hipStream_t stream = at::hip::getCurrentHIPStream().stream();

    // CHIẾN THUẬT: Nếu BS1 & context ngắn -> Chạy single pass để thắng latency. Ngược lại -> Split-KV.
    if (num_seqs == 1 && total_q_tokens == 1 && kv_lens[0] <= 512) {
        size_t shmem = (head_size + 32) * sizeof(float);
        paged_attn_single_v170<<<dim3(1, num_heads), dim3(std::max(head_size, 64)), shmem, stream>>>(
            (hip_bfloat16*)query.data_ptr(), (hip_bfloat16*)key_cache.data_ptr(), (hip_bfloat16*)value_cache.data_ptr(),
            block_tables.data_ptr<int>(), (int)kv_lens[0], (int)query_lens[0],
            (hip_bfloat16*)output.data_ptr(), (int)key_cache.size(2), num_heads, head_size, (int)key_cache.size(1), (float)scale, (float)soft_cap, (int)block_tables.size(1),
            query.stride(0), query.stride(1), query.stride(2), key_cache.stride(0), key_cache.stride(1), key_cache.stride(2), key_cache.stride(3), value_cache.stride(0), value_cache.stride(1), value_cache.stride(2), value_cache.stride(3)
        );
        return output;
    }

    // ĐƯỜNG DẪN SPLIT-KV (FLASH-DECODING)
    int num_splits = (num_seqs == 1 && kv_lens[0] > 512) ? 64 : 32; 
    auto options_f32 = query.options().dtype(torch::kFloat32);
    Tensor p_acc = torch::empty({total_q_tokens, num_heads, num_splits, head_size}, options_f32);
    Tensor p_max = torch::full({total_q_tokens, num_heads, num_splits}, -1e20f, options_f32);
    Tensor p_sum = torch::empty({total_q_tokens, num_heads, num_splits}, options_f32);

    std::vector<int> h_q_start(num_seqs + 1); int curr = 0;
    for(int i=0; i<num_seqs; ++i) { h_q_start[i] = curr; curr += query_lens[i]; }
    h_q_start[num_seqs] = curr;
    auto opts_i32 = torch::TensorOptions().dtype(torch::kInt32).device(device);
    Tensor q_start_gpu = torch::tensor(h_q_start, opts_i32), kv_lens_gpu = torch::tensor(std::vector<int>(kv_lens.begin(), kv_lens.end()), opts_i32), q_lens_gpu = torch::tensor(std::vector<int>(query_lens.begin(), query_lens.end()), opts_i32);

    size_t shmem = (head_size + 32) * sizeof(float);
    paged_attn_split_v170<<<dim3(total_q_tokens, num_heads, num_splits), dim3(std::max(head_size, 64)), shmem, stream>>>(
        (hip_bfloat16*)query.data_ptr(), (hip_bfloat16*)key_cache.data_ptr(), (hip_bfloat16*)value_cache.data_ptr(),
        block_tables.data_ptr<int>(), kv_lens_gpu.data_ptr<int>(), q_lens_gpu.data_ptr<int>(), q_start_gpu.data_ptr<int>(),
        p_acc.data_ptr<float>(), p_max.data_ptr<float>(), p_sum.data_ptr<float>(),
        num_splits, (int)key_cache.size(2), num_heads, head_size, (int)key_cache.size(1), (float)scale, (float)soft_cap, (int)block_tables.size(1), num_seqs,
        query.stride(0), query.stride(1), query.stride(2), key_cache.stride(0), key_cache.stride(1), key_cache.stride(2), key_cache.stride(3), value_cache.stride(0), value_cache.stride(1), value_cache.stride(2), value_cache.stride(3)
    );
    paged_attn_combine_v170<<<dim3(total_q_tokens, num_heads), head_size, 0, stream>>>(p_acc.data_ptr<float>(), p_max.data_ptr<float>(), p_sum.data_ptr<float>(), (hip_bfloat16*)output.data_ptr(), num_splits, head_size, num_heads);

    return output;
}

PYBIND11_MODULE(custom_attn, m) {
    m.def("custom_paged_attn", &custom_paged_attn, "Hybrid Master V170");
}











#include <torch/torch.h>
#include <hip/hip_runtime.h>
#include <hip/hip_bfloat16.h> 
#include <ATen/hip/HIPContext.h> 
#include <vector>
#include <cmath>

using torch::Tensor;

#define WARP_SIZE 64

// -------------------------------------------------------------------------
// Kernel hỗ trợ: Thay thế cho torch::full (Vì torch::full bị cấm)
// -------------------------------------------------------------------------
__global__ void fill_float_kernel(float* ptr, float val, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) ptr[idx] = val;
}

// -------------------------------------------------------------------------
// Common Helpers cho Reduction
// -------------------------------------------------------------------------
__device__ __forceinline__ float warp_reduce_max(float val) {
    for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2)
        val = fmaxf(val, __shfl_xor(val, offset, WARP_SIZE));
    return val;
}

__device__ __forceinline__ float warp_reduce_sum(float val) {
    for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2)
        val += __shfl_xor(val, offset, WARP_SIZE);
    return val;
}

__device__ __forceinline__ float block_reduce_sum_split(float val, float* shared_res) {
    int tid = threadIdx.x; int wid = tid / WARP_SIZE; int lane = tid % WARP_SIZE;
    for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2) val += __shfl_xor(val, offset, WARP_SIZE);
    if (lane == 0) shared_res[wid] = val;
    __syncthreads();
    val = (tid < blockDim.x / WARP_SIZE) ? shared_res[tid] : 0.0f;
    if (wid == 0) for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2) val += __shfl_xor(val, offset, WARP_SIZE);
    if (tid == 0) shared_res[0] = val;
    __syncthreads();
    return shared_res[0];
}

// -------------------------------------------------------------------------
// Kernel 1: Warp-Parallel (Tối ưu BS1 Context ngắn)
// -------------------------------------------------------------------------
__global__ void kernel_warp_parallel_v300(
    const hip_bfloat16* __restrict__ q, const hip_bfloat16* __restrict__ k_cache, const hip_bfloat16* __restrict__ v_cache,
    const int* __restrict__ block_tables, const int* __restrict__ kv_lens, const int* __restrict__ q_lens, const int* __restrict__ q_start_loc,
    hip_bfloat16* __restrict__ output, const int num_kv_heads, const int num_q_heads, const int block_size,
    const float scale, const float soft_cap, const int head_size, const int max_blocks_per_seq, const int num_seqs,
    const long long q_s0, const long long q_s1, const long long q_s2,
    const long long k_s0, const long long k_s1, const long long k_s2, const long long k_s3,
    const long long v_s0, const long long v_s1, const long long v_s2, const long long v_s3
) {
    extern __shared__ float s_mem[];
    float* s_query = s_mem; float* s_max = &s_mem[head_size];
    float* s_sum = &s_mem[head_size + 32]; float* s_acc_combine = &s_mem[head_size + 64];

    const int q_token_idx = blockIdx.x; const int head_idx = blockIdx.y; 
    const int tid = threadIdx.x; const int wid = tid / WARP_SIZE; const int lane = tid % WARP_SIZE;
    const int num_warps = blockDim.x / WARP_SIZE;

    int seq_idx = 0; for (int i = 0; i < num_seqs; i++) { if (q_token_idx >= q_start_loc[i]) seq_idx = i; }
    const int kv_len = kv_lens[seq_idx]; const int causal_limit = kv_len - q_lens[seq_idx] + (q_token_idx - q_start_loc[seq_idx]);
    const int kv_head_idx = head_idx / (num_q_heads / num_kv_heads);

    if (tid < head_size) s_query[tid] = static_cast<float>(q[q_token_idx * q_s0 + head_idx * q_s1 + tid * q_s2]);
    __syncthreads();

    float m_i = -1e20f, d_i = 0.0f; float acc_local[8] = {0.0f};
    const int elements_per_thread = (head_size + WARP_SIZE - 1) / WARP_SIZE;

    for (int i = wid; i < kv_len; i += num_warps) {
        if (i > causal_limit) break;
        const int p_blk = block_tables[seq_idx * max_blocks_per_seq + (i / block_size)];
        const int blk_off = i % block_size;
        float dot = 0.0f;
        for (int e = 0; e < elements_per_thread; e++) {
            int d = lane + e * WARP_SIZE;
            if (d < head_size) dot += s_query[d] * static_cast<float>(k_cache[p_blk * k_s0 + blk_off * k_s1 + kv_head_idx * k_s2 + d * k_s3]) * scale;
        }
        float score = warp_reduce_sum(dot);
        score = __shfl(score, 0, WARP_SIZE);
        if (soft_cap > 0.0f) score = soft_cap * tanhf(score / soft_cap);

        float m_next = fmaxf(m_i, score), p = expf(score - m_next), alpha = expf(m_i - m_next);
        m_i = m_next; d_i = d_i * alpha + p;
        for (int e = 0; e < elements_per_thread; e++) {
            int d = lane + e * WARP_SIZE;
            if (d < head_size) acc_local[e] = acc_local[e] * alpha + p * static_cast<float>(v_cache[p_blk * v_s0 + blk_off * v_s1 + kv_head_idx * v_s2 + d * v_s3]);
        }
    }
    if (lane == 0) { s_max[wid] = m_i; s_sum[wid] = d_i; }
    for (int e = 0; e < elements_per_thread; e++) {
        int d = lane + e * WARP_SIZE;
        if (d < head_size) s_acc_combine[wid * head_size + d] = acc_local[e];
    }
    __syncthreads();
    if (wid == 0) {
        float g_max = -1e20f; for (int w = 0; w < num_warps; w++) g_max = fmaxf(g_max, s_max[w]);
        float g_sum = 0.0f; for (int w = 0; w < num_warps; w++) g_sum += s_sum[w] * expf(s_max[w] - g_max);
        for (int e = 0; e < elements_per_thread; e++) {
            int d = lane + e * WARP_SIZE;
            if (d < head_size) {
                float f_acc = 0.0f; for (int w = 0; w < num_warps; w++) f_acc += s_acc_combine[w * head_size + d] * expf(s_max[w] - g_max);
                output[q_token_idx * num_q_heads * head_size + head_idx * head_size + d] = static_cast<hip_bfloat16>(f_acc / (g_sum + 1e-10f));
            }
        }
    }
}

// -------------------------------------------------------------------------
// Kernel 2: Split-KV Compute (Tối ưu BS1 Context dài)
// -------------------------------------------------------------------------
__global__ void kernel_split_v300(
    const hip_bfloat16* __restrict__ q, const hip_bfloat16* __restrict__ k_cache, const hip_bfloat16* __restrict__ v_cache,
    const int* __restrict__ block_tables, const int* __restrict__ kv_lens, const int* __restrict__ q_lens, const int* __restrict__ q_start_loc,
    float* __restrict__ partial_acc, float* __restrict__ partial_max, float* __restrict__ partial_sum,
    const int num_splits, const int num_kv_heads, const int num_q_heads, const int head_size, const int block_size,
    const float scale, const float soft_cap, const int max_blocks_per_seq, const int num_seqs,
    const long long k_s0, const long long k_s1, const long long k_s2, const long long k_s3,
    const long long v_s0, const long long v_s1, const long long v_s2, const long long v_s3
) {
    const int q_token_idx = blockIdx.x; const int head_idx = blockIdx.y; const int split_idx = blockIdx.z; const int tid = threadIdx.x;
    extern __shared__ float s_mem[];
    float* s_query = s_mem; float* s_res = &s_mem[head_size];

    int seq_idx = 0; for (int i = 0; i < num_seqs; i++) { if (q_token_idx >= q_start_loc[i]) seq_idx = i; }
    const int kv_len = kv_lens[seq_idx]; const int causal_limit = kv_len - q_lens[seq_idx] + (q_token_idx - q_start_loc[seq_idx]);

    if (tid < head_size) s_query[tid] = static_cast<float>(q[q_token_idx * (num_q_heads * head_size) + head_idx * head_size + tid]);
    __syncthreads();

    const int t_per_split = (kv_len + num_splits - 1) / num_splits;
    const int start = split_idx * t_per_split; const int end = min(start + t_per_split, causal_limit + 1);
    if (start >= end) return;

    float m_i = -1e20f, d_i = 0.0f, acc_val = 0.0f;
    const int kv_head_idx = head_idx / (num_q_heads / num_kv_heads);

    for (int i = start; i < end; i++) {
        const int p_blk = block_tables[seq_idx * max_blocks_per_seq + (i / block_size)];
        const int blk_off = i % block_size;
        float dot = (tid < head_size) ? (s_query[tid] * static_cast<float>(k_cache[p_blk * k_s0 + blk_off * k_s1 + kv_head_idx * k_s2 + tid * k_s3]) * scale) : 0.0f;
        float score = block_reduce_sum_split(dot, s_res);
        if (soft_cap > 0.0f) score = soft_cap * tanhf(score / soft_cap);
        float m_next = fmaxf(m_i, score), p = expf(score - m_next), alpha = expf(m_i - m_next);
        m_i = m_next; d_i = d_i * alpha + p;
        if (tid < head_size) acc_val = acc_val * alpha + p * static_cast<float>(v_cache[p_blk * v_s0 + blk_off * v_s1 + kv_head_idx * v_s2 + tid * v_s3]);
    }
    if (tid < head_size) partial_acc[((q_token_idx * num_q_heads + head_idx) * num_splits + split_idx) * head_size + tid] = acc_val;
    if (tid == 0) { int base = (q_token_idx * num_q_heads + head_idx) * num_splits + split_idx; partial_max[base] = m_i; partial_sum[base] = d_i; }
}

// -------------------------------------------------------------------------
// Kernel 3: Combine
// -------------------------------------------------------------------------
__global__ void kernel_combine_v300(const float* partial_acc, const float* partial_max, const float* partial_sum, hip_bfloat16* output, int num_splits, int head_size, int num_q_heads) {
    const int q_token_idx = blockIdx.x; const int head_idx = blockIdx.y; const int tid = threadIdx.x;
    if (tid >= head_size) return;
    const int base = (q_token_idx * num_q_heads + head_idx) * num_splits;
    float g_max = -1e20f; for (int s = 0; s < num_splits; s++) g_max = fmaxf(g_max, partial_max[base + s]);
    float g_sum = 0.0f, f_acc = 0.0f;
    for (int s = 0; s < num_splits; s++) {
        float m_s = partial_max[base + s]; if (m_s < -1e10f) continue;
        float alpha = expf(m_s - g_max); g_sum += alpha * partial_sum[base + s];
        f_acc += alpha * partial_acc[(base + s) * head_size + tid];
    }
    output[q_token_idx * num_q_heads * head_size + head_idx * head_size + tid] = static_cast<hip_bfloat16>(f_acc / (g_sum + 1e-10f));
}

// -------------------------------------------------------------------------
// Host Code: No Forbidden Functions
// -------------------------------------------------------------------------
Tensor custom_paged_attn(
    const Tensor& query, const Tensor& key_cache, const Tensor& value_cache,
    const std::vector<int64_t>& query_lens, const std::vector<int64_t>& kv_lens,
    Tensor block_tables, double scale, double soft_cap = -1.0
) {
    const int num_seqs = query_lens.size();
    const int total_q = query.size(0);
    const int num_h = query.size(1);
    const int head_s = query.size(2);
    
    // 1. Cấp phát Output (Dùng torch::empty là hợp lệ)
    Tensor output = torch::empty({total_q, num_h, head_s}, query.options());
    hipStream_t stream = at::hip::getCurrentHIPStream().stream();

    // 2. Khởi tạo Metadata thủ công (Không dùng torch::tensor)
    std::vector<int> h_q_start(num_seqs + 1); int curr = 0;
    for(int i=0; i<num_seqs; ++i) { h_q_start[i] = curr; curr += query_lens[i]; }
    h_q_start[num_seqs] = curr;

    auto opts_i32 = torch::TensorOptions().dtype(torch::kInt32).device(query.device());
    Tensor q_start_gpu = torch::empty({(long)h_q_start.size()}, opts_i32);
    Tensor kv_lens_gpu = torch::empty({(long)kv_lens.size()}, opts_i32);
    Tensor q_lens_gpu = torch::empty({(long)query_lens.size()}, opts_i32);

    // Copy dữ liệu lên GPU bằng hipMemcpy
    hipMemcpyAsync(q_start_gpu.data_ptr(), h_q_start.data(), h_q_start.size() * sizeof(int), hipMemcpyHostToDevice, stream);
    std::vector<int> kv_lens_i32(kv_lens.begin(), kv_lens.end());
    hipMemcpyAsync(kv_lens_gpu.data_ptr(), kv_lens_i32.data(), kv_lens_i32.size() * sizeof(int), hipMemcpyHostToDevice, stream);
    std::vector<int> q_lens_i32(query_lens.begin(), query_lens.end());
    hipMemcpyAsync(q_lens_gpu.data_ptr(), q_lens_i32.data(), q_lens_i32.size() * sizeof(int), hipMemcpyHostToDevice, stream);

    int avg_kv = 0; for(auto l : kv_lens) avg_kv += (int)l; avg_kv /= num_seqs;

    if (avg_kv <= 512) {
        size_t shmem = (head_s + 64 + 8 * head_s) * sizeof(float);
        kernel_warp_parallel_v300<<<dim3(total_q, num_h), dim3(8 * WARP_SIZE), shmem, stream>>>(
            (hip_bfloat16*)query.data_ptr(), (hip_bfloat16*)key_cache.data_ptr(), (hip_bfloat16*)value_cache.data_ptr(),
            block_tables.data_ptr<int>(), kv_lens_gpu.data_ptr<int>(), q_lens_gpu.data_ptr<int>(), q_start_gpu.data_ptr<int>(),
            (hip_bfloat16*)output.data_ptr(), (int)key_cache.size(2), num_h, (int)key_cache.size(1), (float)scale, (float)soft_cap, head_s, (int)block_tables.size(1), num_seqs,
            query.stride(0), query.stride(1), query.stride(2), key_cache.stride(0), key_cache.stride(1), key_cache.stride(2), key_cache.stride(3), value_cache.stride(0), value_cache.stride(1), value_cache.stride(2), value_cache.stride(3)
        );
    } else {
        int num_splits = 64; 
        auto opts_f32 = query.options().dtype(torch::kFloat32);
        // Thay torch::zeros bằng torch::empty
        Tensor p_acc = torch::empty({total_q, num_h, num_splits, head_s}, opts_f32);
        Tensor p_max = torch::empty({total_q, num_h, num_splits}, opts_f32);
        Tensor p_sum = torch::empty({total_q, num_h, num_splits}, opts_f32);
        
        // Khởi tạo giá trị cho p_max (Thay torch::full bằng Kernel)
        int size_max = total_q * num_h * num_splits;
        fill_float_kernel<<<(size_max + 255) / 256, 256, 0, stream>>>(p_max.data_ptr<float>(), -1e20f, size_max);

        size_t shmem = (head_s + 32) * sizeof(float);
        kernel_split_v300<<<dim3(total_q, num_h, num_splits), dim3(std::max(head_s, 64)), shmem, stream>>>(
            (hip_bfloat16*)query.data_ptr(), (hip_bfloat16*)key_cache.data_ptr(), (hip_bfloat16*)value_cache.data_ptr(),
            block_tables.data_ptr<int>(), kv_lens_gpu.data_ptr<int>(), q_lens_gpu.data_ptr<int>(), q_start_gpu.data_ptr<int>(),
            p_acc.data_ptr<float>(), p_max.data_ptr<float>(), p_sum.data_ptr<float>(),
            num_splits, (int)key_cache.size(2), num_h, head_s, (int)key_cache.size(1), (float)scale, (float)soft_cap, (int)block_tables.size(1), num_seqs,
            key_cache.stride(0), key_cache.stride(1), key_cache.stride(2), key_cache.stride(3), value_cache.stride(0), value_cache.stride(1), value_cache.stride(2), value_cache.stride(3)
        );
        kernel_combine_v300<<<dim3(total_q, num_h), head_s, 0, stream>>>(p_acc.data_ptr<float>(), p_max.data_ptr<float>(), p_sum.data_ptr<float>(), (hip_bfloat16*)output.data_ptr(), num_splits, head_s, num_h);
    }
    return output;
}

PYBIND11_MODULE(custom_attn, m) {
    m.def("custom_paged_attn", &custom_paged_attn, "Strict Compliance V300");
}