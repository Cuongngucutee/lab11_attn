// !!! This is a file automatically generated by hipify!!!
#include <torch/torch.h>
#include <hip/hip_runtime.h>
#include <hip/hip_bfloat16.h> 
#include <ATen/hip/HIPContext.h> 
#include <vector>
#include <cmath>

using torch::Tensor;

#define WARP_SIZE 64

// -------------------------------------------------------------------------
// Kernel hỗ trợ: Thay thế cho torch::full (Vì torch::full bị cấm)
// -------------------------------------------------------------------------
__global__ void fill_float_kernel(float* ptr, float val, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) ptr[idx] = val;
}

// -------------------------------------------------------------------------
// Common Helpers cho Reduction
// -------------------------------------------------------------------------
__device__ __forceinline__ float warp_reduce_max(float val) {
    for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2)
        val = fmaxf(val, __shfl_xor(val, offset, WARP_SIZE));
    return val;
}

__device__ __forceinline__ float warp_reduce_sum(float val) {
    for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2)
        val += __shfl_xor(val, offset, WARP_SIZE);
    return val;
}

__device__ __forceinline__ float block_reduce_sum_split(float val, float* shared_res) {
    int tid = threadIdx.x; int wid = tid / WARP_SIZE; int lane = tid % WARP_SIZE;
    for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2) val += __shfl_xor(val, offset, WARP_SIZE);
    if (lane == 0) shared_res[wid] = val;
    __syncthreads();
    val = (tid < blockDim.x / WARP_SIZE) ? shared_res[tid] : 0.0f;
    if (wid == 0) for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2) val += __shfl_xor(val, offset, WARP_SIZE);
    if (tid == 0) shared_res[0] = val;
    __syncthreads();
    return shared_res[0];
}

// -------------------------------------------------------------------------
// Kernel 1: Warp-Parallel (Tối ưu BS1 Context ngắn)
// -------------------------------------------------------------------------
__global__ void kernel_warp_parallel_v300(
    const hip_bfloat16* __restrict__ q, const hip_bfloat16* __restrict__ k_cache, const hip_bfloat16* __restrict__ v_cache,
    const int* __restrict__ block_tables, const int* __restrict__ kv_lens, const int* __restrict__ q_lens, const int* __restrict__ q_start_loc,
    hip_bfloat16* __restrict__ output, const int num_kv_heads, const int num_q_heads, const int block_size,
    const float scale, const float soft_cap, const int head_size, const int max_blocks_per_seq, const int num_seqs,
    const long long q_s0, const long long q_s1, const long long q_s2,
    const long long k_s0, const long long k_s1, const long long k_s2, const long long k_s3,
    const long long v_s0, const long long v_s1, const long long v_s2, const long long v_s3
) {
    extern __shared__ float s_mem[];
    float* s_query = s_mem; float* s_max = &s_mem[head_size];
    float* s_sum = &s_mem[head_size + 32]; float* s_acc_combine = &s_mem[head_size + 64];

    const int q_token_idx = blockIdx.x; const int head_idx = blockIdx.y; 
    const int tid = threadIdx.x; const int wid = tid / WARP_SIZE; const int lane = tid % WARP_SIZE;
    const int num_warps = blockDim.x / WARP_SIZE;

    int seq_idx = 0; for (int i = 0; i < num_seqs; i++) { if (q_token_idx >= q_start_loc[i]) seq_idx = i; }
    const int kv_len = kv_lens[seq_idx]; const int causal_limit = kv_len - q_lens[seq_idx] + (q_token_idx - q_start_loc[seq_idx]);
    const int kv_head_idx = head_idx / (num_q_heads / num_kv_heads);

    if (tid < head_size) s_query[tid] = static_cast<float>(q[q_token_idx * q_s0 + head_idx * q_s1 + tid * q_s2]);
    __syncthreads();

    float m_i = -1e20f, d_i = 0.0f; float acc_local[8] = {0.0f};
    const int elements_per_thread = (head_size + WARP_SIZE - 1) / WARP_SIZE;

    for (int i = wid; i < kv_len; i += num_warps) {
        if (i > causal_limit) break;
        const int p_blk = block_tables[seq_idx * max_blocks_per_seq + (i / block_size)];
        const int blk_off = i % block_size;
        float dot = 0.0f;
        for (int e = 0; e < elements_per_thread; e++) {
            int d = lane + e * WARP_SIZE;
            if (d < head_size) dot += s_query[d] * static_cast<float>(k_cache[p_blk * k_s0 + blk_off * k_s1 + kv_head_idx * k_s2 + d * k_s3]) * scale;
        }
        float score = warp_reduce_sum(dot);
        score = __shfl(score, 0, WARP_SIZE);
        if (soft_cap > 0.0f) score = soft_cap * tanhf(score / soft_cap);

        float m_next = fmaxf(m_i, score), p = expf(score - m_next), alpha = expf(m_i - m_next);
        m_i = m_next; d_i = d_i * alpha + p;
        for (int e = 0; e < elements_per_thread; e++) {
            int d = lane + e * WARP_SIZE;
            if (d < head_size) acc_local[e] = acc_local[e] * alpha + p * static_cast<float>(v_cache[p_blk * v_s0 + blk_off * v_s1 + kv_head_idx * v_s2 + d * v_s3]);
        }
    }
    if (lane == 0) { s_max[wid] = m_i; s_sum[wid] = d_i; }
    for (int e = 0; e < elements_per_thread; e++) {
        int d = lane + e * WARP_SIZE;
        if (d < head_size) s_acc_combine[wid * head_size + d] = acc_local[e];
    }
    __syncthreads();
    if (wid == 0) {
        float g_max = -1e20f; for (int w = 0; w < num_warps; w++) g_max = fmaxf(g_max, s_max[w]);
        float g_sum = 0.0f; for (int w = 0; w < num_warps; w++) g_sum += s_sum[w] * expf(s_max[w] - g_max);
        for (int e = 0; e < elements_per_thread; e++) {
            int d = lane + e * WARP_SIZE;
            if (d < head_size) {
                float f_acc = 0.0f; for (int w = 0; w < num_warps; w++) f_acc += s_acc_combine[w * head_size + d] * expf(s_max[w] - g_max);
                output[q_token_idx * num_q_heads * head_size + head_idx * head_size + d] = static_cast<hip_bfloat16>(f_acc / (g_sum + 1e-10f));
            }
        }
    }
}

// -------------------------------------------------------------------------
// Kernel 2: Split-KV Compute (Tối ưu BS1 Context dài)
// -------------------------------------------------------------------------
__global__ void kernel_split_v300(
    const hip_bfloat16* __restrict__ q, const hip_bfloat16* __restrict__ k_cache, const hip_bfloat16* __restrict__ v_cache,
    const int* __restrict__ block_tables, const int* __restrict__ kv_lens, const int* __restrict__ q_lens, const int* __restrict__ q_start_loc,
    float* __restrict__ partial_acc, float* __restrict__ partial_max, float* __restrict__ partial_sum,
    const int num_splits, const int num_kv_heads, const int num_q_heads, const int head_size, const int block_size,
    const float scale, const float soft_cap, const int max_blocks_per_seq, const int num_seqs,
    const long long k_s0, const long long k_s1, const long long k_s2, const long long k_s3,
    const long long v_s0, const long long v_s1, const long long v_s2, const long long v_s3
) {
    const int q_token_idx = blockIdx.x; const int head_idx = blockIdx.y; const int split_idx = blockIdx.z; const int tid = threadIdx.x;
    extern __shared__ float s_mem[];
    float* s_query = s_mem; float* s_res = &s_mem[head_size];

    int seq_idx = 0; for (int i = 0; i < num_seqs; i++) { if (q_token_idx >= q_start_loc[i]) seq_idx = i; }
    const int kv_len = kv_lens[seq_idx]; const int causal_limit = kv_len - q_lens[seq_idx] + (q_token_idx - q_start_loc[seq_idx]);

    if (tid < head_size) s_query[tid] = static_cast<float>(q[q_token_idx * (num_q_heads * head_size) + head_idx * head_size + tid]);
    __syncthreads();

    const int t_per_split = (kv_len + num_splits - 1) / num_splits;
    const int start = split_idx * t_per_split; const int end = min(start + t_per_split, causal_limit + 1);
    if (start >= end) return;

    float m_i = -1e20f, d_i = 0.0f, acc_val = 0.0f;
    const int kv_head_idx = head_idx / (num_q_heads / num_kv_heads);

    for (int i = start; i < end; i++) {
        const int p_blk = block_tables[seq_idx * max_blocks_per_seq + (i / block_size)];
        const int blk_off = i % block_size;
        float dot = (tid < head_size) ? (s_query[tid] * static_cast<float>(k_cache[p_blk * k_s0 + blk_off * k_s1 + kv_head_idx * k_s2 + tid * k_s3]) * scale) : 0.0f;
        float score = block_reduce_sum_split(dot, s_res);
        if (soft_cap > 0.0f) score = soft_cap * tanhf(score / soft_cap);
        float m_next = fmaxf(m_i, score), p = expf(score - m_next), alpha = expf(m_i - m_next);
        m_i = m_next; d_i = d_i * alpha + p;
        if (tid < head_size) acc_val = acc_val * alpha + p * static_cast<float>(v_cache[p_blk * v_s0 + blk_off * v_s1 + kv_head_idx * v_s2 + tid * v_s3]);
    }
    if (tid < head_size) partial_acc[((q_token_idx * num_q_heads + head_idx) * num_splits + split_idx) * head_size + tid] = acc_val;
    if (tid == 0) { int base = (q_token_idx * num_q_heads + head_idx) * num_splits + split_idx; partial_max[base] = m_i; partial_sum[base] = d_i; }
}

// -------------------------------------------------------------------------
// Kernel 3: Combine
// -------------------------------------------------------------------------
__global__ void kernel_combine_v300(const float* partial_acc, const float* partial_max, const float* partial_sum, hip_bfloat16* output, int num_splits, int head_size, int num_q_heads) {
    const int q_token_idx = blockIdx.x; const int head_idx = blockIdx.y; const int tid = threadIdx.x;
    if (tid >= head_size) return;
    const int base = (q_token_idx * num_q_heads + head_idx) * num_splits;
    float g_max = -1e20f; for (int s = 0; s < num_splits; s++) g_max = fmaxf(g_max, partial_max[base + s]);
    float g_sum = 0.0f, f_acc = 0.0f;
    for (int s = 0; s < num_splits; s++) {
        float m_s = partial_max[base + s]; if (m_s < -1e10f) continue;
        float alpha = expf(m_s - g_max); g_sum += alpha * partial_sum[base + s];
        f_acc += alpha * partial_acc[(base + s) * head_size + tid];
    }
    output[q_token_idx * num_q_heads * head_size + head_idx * head_size + tid] = static_cast<hip_bfloat16>(f_acc / (g_sum + 1e-10f));
}

// -------------------------------------------------------------------------
// Host Code: No Forbidden Functions
// -------------------------------------------------------------------------
Tensor custom_paged_attn(
    const Tensor& query, const Tensor& key_cache, const Tensor& value_cache,
    const std::vector<int64_t>& query_lens, const std::vector<int64_t>& kv_lens,
    Tensor block_tables, double scale, double soft_cap = -1.0
) {
    const int num_seqs = query_lens.size();
    const int total_q = query.size(0);
    const int num_h = query.size(1);
    const int head_s = query.size(2);
    
    // 1. Cấp phát Output (Dùng torch::empty là hợp lệ)
    Tensor output = torch::empty({total_q, num_h, head_s}, query.options());
    hipStream_t stream = at::hip::getCurrentHIPStream().stream();

    // 2. Khởi tạo Metadata thủ công (Không dùng torch::tensor)
    std::vector<int> h_q_start(num_seqs + 1); int curr = 0;
    for(int i=0; i<num_seqs; ++i) { h_q_start[i] = curr; curr += query_lens[i]; }
    h_q_start[num_seqs] = curr;

    auto opts_i32 = torch::TensorOptions().dtype(torch::kInt32).device(query.device());
    Tensor q_start_gpu = torch::empty({(long)h_q_start.size()}, opts_i32);
    Tensor kv_lens_gpu = torch::empty({(long)kv_lens.size()}, opts_i32);
    Tensor q_lens_gpu = torch::empty({(long)query_lens.size()}, opts_i32);

    // Copy dữ liệu lên GPU bằng hipMemcpy
    hipMemcpyAsync(q_start_gpu.data_ptr(), h_q_start.data(), h_q_start.size() * sizeof(int), hipMemcpyHostToDevice, stream);
    std::vector<int> kv_lens_i32(kv_lens.begin(), kv_lens.end());
    hipMemcpyAsync(kv_lens_gpu.data_ptr(), kv_lens_i32.data(), kv_lens_i32.size() * sizeof(int), hipMemcpyHostToDevice, stream);
    std::vector<int> q_lens_i32(query_lens.begin(), query_lens.end());
    hipMemcpyAsync(q_lens_gpu.data_ptr(), q_lens_i32.data(), q_lens_i32.size() * sizeof(int), hipMemcpyHostToDevice, stream);

    int avg_kv = 0; for(auto l : kv_lens) avg_kv += (int)l; avg_kv /= num_seqs;

    if (avg_kv <= 512) {
        size_t shmem = (head_s + 64 + 8 * head_s) * sizeof(float);
       hipLaunchKernelGGL(( kernel_warp_parallel_v300), dim3(dim3(total_q, num_h)), dim3(dim3(8 * WARP_SIZE)), shmem, stream, 
            (hip_bfloat16*)query.data_ptr(), (hip_bfloat16*)key_cache.data_ptr(), (hip_bfloat16*)value_cache.data_ptr(),
            block_tables.data_ptr<int>(), kv_lens_gpu.data_ptr<int>(), q_lens_gpu.data_ptr<int>(), q_start_gpu.data_ptr<int>(),
            (hip_bfloat16*)output.data_ptr(), (int)key_cache.size(2), num_h, (int)key_cache.size(1), (float)scale, (float)soft_cap, head_s, (int)block_tables.size(1), num_seqs,
            query.stride(0), query.stride(1), query.stride(2), key_cache.stride(0), key_cache.stride(1), key_cache.stride(2), key_cache.stride(3), value_cache.stride(0), value_cache.stride(1), value_cache.stride(2), value_cache.stride(3)
        );
    } else {
        int num_splits = 64; 
        auto opts_f32 = query.options().dtype(torch::kFloat32);
        // Thay torch::zeros bằng torch::empty
        Tensor p_acc = torch::empty({total_q, num_h, num_splits, head_s}, opts_f32);
        Tensor p_max = torch::empty({total_q, num_h, num_splits}, opts_f32);
        Tensor p_sum = torch::empty({total_q, num_h, num_splits}, opts_f32);
        
        // Khởi tạo giá trị cho p_max (Thay torch::full bằng Kernel)
        int size_max = total_q * num_h * num_splits;
       hipLaunchKernelGGL(( fill_float_kernel), dim3((size_max + 255) / 256), dim3(256), 0, stream, p_max.data_ptr<float>(), -1e20f, size_max);

        size_t shmem = (head_s + 32) * sizeof(float);
       hipLaunchKernelGGL(( kernel_split_v300), dim3(dim3(total_q, num_h, num_splits)), dim3(dim3(std::max(head_s, 64))), shmem, stream, 
            (hip_bfloat16*)query.data_ptr(), (hip_bfloat16*)key_cache.data_ptr(), (hip_bfloat16*)value_cache.data_ptr(),
            block_tables.data_ptr<int>(), kv_lens_gpu.data_ptr<int>(), q_lens_gpu.data_ptr<int>(), q_start_gpu.data_ptr<int>(),
            p_acc.data_ptr<float>(), p_max.data_ptr<float>(), p_sum.data_ptr<float>(),
            num_splits, (int)key_cache.size(2), num_h, head_s, (int)key_cache.size(1), (float)scale, (float)soft_cap, (int)block_tables.size(1), num_seqs,
            key_cache.stride(0), key_cache.stride(1), key_cache.stride(2), key_cache.stride(3), value_cache.stride(0), value_cache.stride(1), value_cache.stride(2), value_cache.stride(3)
        );
       hipLaunchKernelGGL(( kernel_combine_v300), dim3(dim3(total_q, num_h)), dim3(head_s), 0, stream, p_acc.data_ptr<float>(), p_max.data_ptr<float>(), p_sum.data_ptr<float>(), (hip_bfloat16*)output.data_ptr(), num_splits, head_s, num_h);
    }
    return output;
}

PYBIND11_MODULE(custom_attn, m) {
    m.def("custom_paged_attn", &custom_paged_attn, "Strict Compliance V300");
}